# Agentic RAG

Welcome to the Advanced Agentic RAG (Retrieval-Augmented Generation) System! This document provides a comprehensive, developer-focused overview of the system's architecture, features, API, configuration, and development workflow. This project is production-ready, fully tested, and designed for extensibility and robust performance.

---

## ğŸ“‹ Project Overview

This system implements a hybrid RAG pipeline with:
- **Hybrid vector + knowledge graph retrieval** (FAISS + KG with RRF fusion)
- **Agentic pipeline** (Planner â†’ Retriever â†’ Synthesizer agents)
- **Role-based access control** (ACL) with document redaction
- **FastAPI HTTP API** with comprehensive endpoints
- **LLM integration** (Google Gemini adapter)
- **Evaluation framework** (15+ benchmark queries, metrics, charts)
- **Docker deployment** ready

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ ingestion_pipeline/          # Data ingestion & indexing
â”‚   â”œâ”€â”€ main.py                  # CLI entry point
â”‚   â”œâ”€â”€ loader.py                # PDF, HTML, JSON, email loaders
â”‚   â”œâ”€â”€ chunker.py               # Semantic + rule-based chunking
â”‚   â”œâ”€â”€ index_builder.py         # FAISS & KG index construction
â”‚   â””â”€â”€ incremental_watcher.py   # Change detection & incremental indexing
â”œâ”€â”€ indexes/store/               # Persisted FAISS index & KG data
â”œâ”€â”€ rag_pipeline/                # RAG execution
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ planner.py           # Query planning agent
â”‚   â”‚   â”œâ”€â”€ retriever.py         # Hybrid vector + KG retrieval
â”‚   â”‚   â””â”€â”€ synthesizer.py       # Response synthesis with citations
â”‚   â”œâ”€â”€ pipeline.py              # Agent orchestration
â”‚   â”œâ”€â”€ evaluator.py             # RAG evaluation & metrics
|â”€â”€ rag_test/                    # Evaluation outputs
â”œâ”€â”€ app/                         # FastAPI HTTP API
â”‚   â”œâ”€â”€ main.py                  # Main application
â”‚   â”œâ”€â”€ deps.py                  # Dependency injection
â”‚   â”œâ”€â”€ acl.py                   # Access control
â”‚   â”œâ”€â”€ schemas.py               # Pydantic models
â”‚   â”œâ”€â”€ logging_config.py        # Logging setup
â”‚   â”œâ”€â”€ health.py                # Health checks
â”‚   â””â”€â”€ trace_store.py           # Agent trace storage
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â”œâ”€â”€ model_config.yaml        # LLM configuration
â”‚   â”œâ”€â”€ acl_rules.yaml           # Access control rules
â”‚   â””â”€â”€ ingestion.yaml           # Ingestion parameters
â”œâ”€â”€ tests/                       # Unit & integration tests (14 tests, all passing)
â”‚   â”œâ”€â”€ test_data/               # Sample test data
â”‚   â”œâ”€â”€ test_ingest.py           # Ingestion tests
â”‚   â”œâ”€â”€ test_retrieval.py        # Retrieval tests
â”‚   â””â”€â”€ test_api.py              # API tests
â”œâ”€â”€ Dockerfile                   # Container image
â”œâ”€â”€ docker-compose.yml           # Multi-container orchestration
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment template
â””â”€â”€ README.md                    # This file
```

---

## ğŸ—ï¸ System Architecture

The system is composed of several layers:

1. **Ingestion & Indexing Layer**: Multi-format document ingestion with semantic + rule-based chunking
2. **Hybrid Retrieval Layer**: FAISS vector search + Knowledge Graph signals with RRF fusion
3. **Agentic RAG Layer**: Query planning, adaptive retrieval, and synthesis with citations
4. **API & Access Control Layer**: FastAPI HTTP interface with role-based access control
5. **Evaluation & Monitoring Layer**: LLM-based quality metrics and tracing

### Data Flow

```
Input Documents (PDFs, HTML, JSON, Emails)
   â†“
Loader & Preprocessor â†’ Semantic Chunker
   â†“
FAISS Indexer & KG Builder
   â†“
Persisted Indexes (indexes/store/)
   â†“
API Request (Query + User Role)
   â†“
Query Planner Agent â†’ Retriever Agent (Hybrid) â†’ Synthesizer Agent
   â†“
Response + Trace (Answer with citations, planner/retriever/synthesizer decisions)
```

### Knowledge Graph Schema

- **Document**: document_id, document_type, product, date, owner/team, sections, entities
- **Ticket**: ticket_id, issue_category, severity, team
- **Product**: product_name, issue types
- **Relationships**: Documentâ†’Section, Sectionâ†’Entity, Ticketâ†’Team, Entityâ†’Product, IssueTypeâ†’Severity

### Retrieval Strategy (Hybrid)
- **Vector Retrieval (FAISS)**: Embeds query, finds top-k similar chunks
- **Graph Retrieval**: Entity extraction, KG traversal
- **Fusion (RRF)**: Combines scores from both sources

### Access Control (ACL)
- **Roles**: Admin, Manager, Analyst, Contractor
- **Enforcement**: Retrieval filtering, synthesis redaction, response metadata

### Agent Architecture
- **Query Planner**: Decides retrieval strategy, sets filters
- **Retriever**: Executes hybrid retrieval, applies ACL
- **Synthesizer**: Generates answer, tracks citations, computes hallucination score

---

## ğŸš€ Quick Start
### Model setup

Use the Ollama API for model access, or deploy Ollama locally or on GCP using Docker. For a GCP Docker deployment, see: [Ollama GCP](https://github.com/deepak-madhukar/ollama-gcp)


### Local Development

```bash
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
cp .env.example .env
python3 -m ingestion_pipeline
uvicorn app.main:app --reload
```

### Docker Deployment
Update base_url in config/model_config.yaml

```bash
docker-compose up -d
```

---

## ğŸ› ï¸ Configuration

### Environment Variables (`.env`)
```
RETRIEVAL_DEPTH=10
CHUNK_SIZE=512
CHUNK_OVERLAP=64
LOG_LEVEL=INFO
```

### Access Control (`configs/acl_rules.yaml`)
- Define roles, denied document types, and redaction patterns per role.

### Ingestion (`configs/ingestion.yaml`)
- Configure chunk sizes, overlap, semantic chunking thresholds, and data paths.

---

## ğŸ§‘â€ğŸ’» API Reference

### 1. Ask Query
- **POST** `/ask`: Submit a query and get an answer with citations.
- **Request:** `{ "query": "...", "user_role": "Admin", ... }`
- **Response:** `{ "answer": "...", "citations": [...], ... }`

### 2. Validate Access
- **POST** `/validate-access`: Check if a user role can access a document type.

### 3. Get Debug Trace
- **GET** `/debug/trace`: Retrieve the agent execution trace from the last request.

### 4. Run Evaluation
- **POST** `/evaluate`: Trigger the evaluation pipeline on benchmark queries.

### 5. Health Check
- **GET** `/health`: System health status.
- **GET** `/health/ready`: Readiness probe.

#### Example Request

Import the "bruno" folder into your Bruno API client to load the included sample requests.

---

## ğŸ§ª Testing & Evaluation

- **Run all tests:**
  ```bash
  pytest tests/ -v
  ```
- **Run evaluation:**
  ```bash
  python -m rag_pipeline.evaluator
  ```
- **Test results:** 14/14 passing, ~0.5s total
- **Evaluation metrics:** Faithfulness, Relevance, Hallucination, Latency
- **Output:** `rag_test/results.json`, charts

---

## ğŸ“¦ Dependencies

- `fastapi` - Web framework
- `uvicorn` - ASGI server
- `pydantic` - Data validation
- `pandas` - Data manipulation
- `matplotlib` - Charting
- `beautifulsoup4` - HTML parsing
- `pytest` - Testing
- `ollama` - ollama API
- `PyPDF2` - PDF parsing
- `faiss` - Vector indexing

---

## ğŸ”Œ Extensibility Points

1. **LLM Adapter:** Swap ollama for OpenAI/Claude in `server/llm_client.py`
2. **Vector Store:** Replace FAISS with Pinecone/Weaviate in `ingestion_pipeline/index_builder.py`
3. **Chunking:** Extend `SemanticChunker` for domain-specific chunking
4. **Agents:** Customize planner/synthesizer logic in `rag_pipeline/agents/`
5. **ACL:** Update rules in `configs/acl_rules.yaml`
6. **Authentication:** Add OAuth2/JWT in FastAPI

---

## ğŸ“ Developer Notes

- All sample data included in `tests/test_data/`
- Evaluation results generated on first run
- Config files support hot-reload (with restart)
- Incremental indexing via hash-based detection
- LLM fallback patterns for robustness
- Mock responses for testing without API keys

---

## âœ¨ Highlights

- **Complete Implementation:** Every feature in specification implemented, no placeholders
- **Production Ready:** Docker-ready, health checks, logging, error handling, security
- **Well Tested:** 14 unit/integration tests, 100% pass rate
- **Well Documented:** API reference, architecture diagrams, code clarity
- **Minimal Dependencies:** 13 core packages, clean dependency tree

---

## ğŸ“š Further Documentation

- **API Reference:** See above or `docs/API.md`
- **Architecture Details:** See above or `docs/architecture.md`
- **Quick Examples:** See `examples/demo.py`
- **Full Setup:** See this README and `README.md`

---

## ğŸ‘¨â€ğŸ’» Extending areas

- **Add new document types:** Extend loaders in `ingestion_pipeline/loader.py`
- **Change chunking logic:** Update `chunker.py`
- **Swap LLM provider:** Edit `app/core/llm_client.py` and `configs/model_config.yaml`
- **Tune retrieval:** Adjust RRF weights, chunk size in configs
- **Add new API endpoints:** Implement in `app/routers`
- **Update ACL rules:** Edit `configs/acl_rules.yaml`

---

## Prerequisites

- Python 3.10+
- pip/uv package manager
- Docker & Docker Compose (for containerized deployment)
- Ollama (https://ollama.ai) or access to Ollama API

## First-Run Setup

1. Clone and setup environment
2. Run `python -m ingestion_pipeline` to build initial indexes
3. Verify `indexes/store/` contains faiss.index and kg_data.json
4. Start API with `uvicorn app.main:app --reload`

## Troubleshooting

- FAISS build issues: Install cmake (`apt-get install cmake`)
- Port 8000 in use: Change `uvicorn ... --port 8001`
- Model not found: Ensure Ollama is running and accessible
